*This document is part of a series of notes for CS4051 Human factors that can be found [here](https://github.com/nating/personal-notes/blob/master/fourth-year/human-factors/notes)*

## Evaluation

**Evaluation** involves testing the usability and functionality of a system.

Evaluation should be considered at every stage of the design life cycle.

All evaluations need goals and questions to guide them.

An example of the questions needed to guide an evaluation are:  
"The goal of finding out why some customers prefer to purchase paper train tickets rather than e-tickets can be broken down into sub-questions:
– What are customers’ attitudes to these new tickets?
– Are they concerned about them being accepted?
– Is the interface for obtaining them poor?"

The **Evaluation Approach** influences the **Evaluation Method**.

Practical issues with evaluation involve:  
* Selecting users
* Staying on budget
* Staying on schedule
* Finding evaluators
* Selecting equipment

There are ethical issues concerning evaluation, and an informed consent form needs to be developed.

Participants in an evaluation have a right to:
* Know the goals of the study
* Know what will happen to the findings
* Privacy of personal information
* Leave when they wish
* Be treated politely

Considerations to be made when evaluating include:
* Can this study be replicated? (Is it reliable)
* Is the process creating biases?
* Can the findings be generalised?
* Is the environment influencing the findings?

|Study|Advantages|Disadvantages|Appropriate when|
|---|---|---|---|
|Laboratory Studies|Specialist equipment available & uninterrupted environment|"Lack of context"|It is impractical to observe features of interest in a real setting|
|Field Studies|Natural Environment, context|Distractions|Appropriate where context is crucial or evaluation occurs over a long period|

**Heuristic Evaluation** involves examining a design to see if usability criteria (*heuristics*) are violated.

Heuristic Evaluation is said to 'debug' design.

#### Nielsen's Heuristics
1. Visibility of system status
2. Match between system & the real world
3. User control & freedom
4. Consistency & Standards
5. Error Prevention
6. Recognition rather than recall
7. Flexibility & efficiency of use
8. Aesthetic and minimalist design
9. Help users recognise, diagnose, and recover from errors
10. Help & document

In order to evaluate a design, there must be a simulation, prototype or full implementation.

In experimental evaluation, specific aspects of interactive behaviour are evaluated by differing some experimental conditions while also having control variables.

In experimental evaluation, a sufficient sample of subjects are representative of users.

Examples of **Independent Variables** (*IVs*)are interface style and number of menu items.

Examples of **Dependent Variables** (*DVs*) are time taken and number of errors.

The **Null Hypothesis** states that there is no difference between conditions.

When doing experimental evaluation, the conditions can be changed 'within subjects' or 'between subjects'.

When conditions are changed within subjects, the process is cheaper and less likely to suffer from user variation, but a transfer of learning is possible from the first condition to the second.

When conditions are changed between subjects, there is no possibility for transfer of learning but more users are required and the variation of users can bias the results.

After doing evaluation, there needs to be an analysis of the data.

**Observation Methods** include:
* Think Aloud
* Cooperative Evaluation
* Protocol Analysis

The **Think Aloud** observation method / protocol involves a user describing what they are doing, why they are doing it and what they think is happening.

Advantages: Think aloud requires little expertise, can provide useful insight and can show how the system is actually used.

Disadvantages: Think aloud can be subjective and selective and the act of describing may alter the performance of the task.

Think aloud is very useful because
* The observer can correlate the actions and statements of the participant.
* It results in rapid high-quality user feedback.
* Data is retrieved from both observing what the subject is doing & hearing what the subject is trying to do.
* The experiment may easily be steered by the observer.
* The presence of two people allows meaningful direct dialogue.
* Quantitative content analysis can be done by examining video evidence after the experiment.

When performing the Think Aloud protocol, make sure that the user realises that the interface is under scrutiny (not them).

Note taking is still important when videoing the experiment as there is no way of clarifying ambiguities when watching a video.

The observer should try to help the subject as little as they can.

**Cooperative Evaluation** is a variation of Think Aloud where the user collaborates in the evaluation. Both the user and evaluator are encouraged to ask each other questions throughout.

An advantage of Cooperative Evaluation over Think Aloud is that the user is encouraged to criticise the system.

**Protocol Analysis** is the record of the evaluation session.

|Protocol Analysis method|Advantages|Disadvantages|
|---|---|---|
|Paper & Pencil|Cheap|Limited to writing speed|
|Audio|Good for thinking aloud|No record of things that don't make a sound|
|Video|Accurate & Realistic|Obtrusive & needs special equipment|
|Computer Logging|Automatic & Unobtrusive|Large amounts of data that can be difficult to analyse|
|User notebooks|Useful insights|Subjective|
|Post task walkthroughs|Analyst has time to focus on incidents & avoids excessive interruption of tasks|Lack of freshness & post-hoc interpretation of events|

A mixture of protocol analysis methods are used in practice.

**Query Techniques** include **Interviews** & **Questionnaires**.

**Interviews** can be varied to suit context, issues can be explored fully, and can elicit user views and identify unanticipated problems. But they are also subjective and time consuming.

**Questionnaires** are quick & reach large user groups, but are less flexible, need to be carefully designed and don't afford the same probing as interviews.

The **System Usability Scale** is ten questions that users are asked to scale from Strongly Agree to Strongly Disagree.  
- I think that I would like to use this system frequently.
- I found the system unnecessarily complex.
- I thought the system was easy to use.
- I think that I would need the support of a technical person to be able to use this system.
- I found the various functions in this system were well integrated.
- I thought there was too much inconsistency in this system.
- I would imagine that most people would learn to use this system very quickly.
- I found the system very cumbersome to use.
- I felt very confident using the system.
- I needed to learn a lot of things before I could get going with this system.

When **choosing your evaluation method**, one should ask oneself:  
* When in the cycle is the evaluation to be carried out?
* Should the evaluation be a field evaluation or a laboratory evaluation?
* How objective should the technique be?
* Should the measures be qualitative or quantitative?
* What level of information is required?
* Should the evaluation be obtrusive or unobtrusive?
* What resources are available? (Time? Subjects? Equipment? Expertise?)

**HCT** stands for the **Human Computer Trust Scale**.

The Human Computer Trust scale is a list of points to be ticked off for a system about how users feel about it.
